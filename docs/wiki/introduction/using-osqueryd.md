`osqueryd` is the host monitoring daemon that allows you to **schedule** queries. The daemon aggregates query results over time and generates logs, which indicate state change according to each query.

## Query schedule

The primary daemon feature is executing a query schedule. This schedule is defined in an osquery configuration and includes a list of semi-broad queries and their interval. The interval is an approximate time to run the query.

```json
{
  "usb_devices": {
    "query": "select * from usb_devices",
    "interval": 60
  }
}
```

This simple **usb_devices** query will run approximately every 60 seconds on the host running osqueryd.

## Configuration

By default, queries are read from a local JSON file. The default config plugin, "filesystem", reads "/etc/osquery/osquery.conf" on Linux and "/var/osquery/osquery.conf" on OS X. You may include additional config files in a directory "/etc/osquery/osquery.conf.d/" or "/var/osquery/osquery.conf.d/" respectively. If you set `--config_path=` to a different path, osquery will use that path and search for a "filename" + ".d/" directory.

Here is an example config that includes options and the query schedule:

```json
{
  "options": {
    "host_identifier": "hostname",
    "schedule_splay_percent": 10
  },
  "schedule": {
    "macosx_kextstat": {
      "query": "SELECT * FROM kernel_extensions;",
      "interval": 10
    },
    "foobar": {
      "query": "SELECT foo, bar, pid FROM foobar_table;",
      "interval": 600
    }
  }
}
```

This config tells osqueryd to schedule two queries, **macosx_kextstat** and **foobar**:

* the schedule keys must be unique
* the "interval" specifies query frequency, in seconds

The first query will document changes to an OS X host's kernel extensions, with a query interval of 10 seconds. Consider using osquery's [performance tooling](deployment/performance-safety) to understand the performance impact for each query.

The results of your query are cached on disk via [RocksDB](http://rocksdb.org/). On first query run, all of the results are stored in RocksDB. On subsequent runs, only result-set changes are logged to RocksDB.

## Logs

Also by default, log results are written as JSON to disk. The default logger plugin "filesystem" writes query result actions to "/var/log/osquery/osqueryd.results.log", as well as status logs to the same directory.

There are two sets of logs:

- status logs (info, warning, error, and fatal)
- query results logs

If you run osqueryd in a verbose mode then peek at "/var/log/osquery/":

```sh
$ ls -l /var/log/osquery/
total 24
lrwxr-xr-x   1 root  wheel    77 Sep 30 17:37 osqueryd.INFO -> osqueryd.INFO.20140930
-rw-------   1 root  wheel  1226 Sep 30 17:37 osqueryd.INFO.20140930
-rw-------   1 root  wheel   388 Sep 30 17:37 osqueryd.results.log
```

### Status logs

Status logs are generated by the [glog logging framework](https://code.google.com/p/google-glog/).

As the above directory listing reveals,
"osqueryd.INFO" is a symlink to the most recent execution's INFO log.
The same is true for the WARNING, ERROR and FATAL logs.

For more information on the format of glog logs, please refer to the [glog documentation](http://google-glog.googlecode.com/svn/trunk/doc/glog.html).

### Results logs

The results of your scheduled queries are logged to the "results log".
Each log line is a JSON string that indicates what data has been added/removed by which query.
There are two format options, "single", or event, and "batched".

## Results

### Event format

Event is the default result format. Each log line represents a state change.
This format works best for log aggregation systems like Logstash or Splunk.

Example output of `SELECT name, path, pid FROM processes;` (whitespace added for readability):

```json
{
  "action": "added",
  "columns": {
    "name": "osqueryd",
    "path": "/usr/local/bin/osqueryd",
    "pid": "97830"
  },
  "name": "processes",
  "hostname": "hostname.local",
  "calendarTime": "Tue Sep 30 17:37:30 2014",
  "unixTime": "1412123850"
}
```

```json
{
  "action": "removed",
  "columns": {
    "name": "osqueryd",
    "path": "/usr/local/bin/osqueryd",
    "pid": "97650"
  },
  "name": "processes",
  "hostname": "hostname.local",
  "calendarTime": "Tue Sep 30 17:37:30 2014",
  "unixTime": "1412123850"
}
```

This tells us that a binary called "osqueryd" was stopped and a new binary with the same name was started (note the different pids). The data is generated by keeping a cache of previous query results and only logging when the cache changes. If no new processes are started or stopped, the query won't log any results.

### Batch format

If a query identifies multiple state changes, the batched format will include all results in a single log line. If you're programmatically parsing lines and loading them into a backend datastore, this is probably the best solution.

To enable batch log lines, launch osqueryd with the `--log_result_events=false` argument.

Example output of `SELECT name, path, pid FROM processes;` (whitespace added for readability):

```json
{
  "diffResults": {
    "added": [
      {
        "name": "osqueryd",
        "path": "/usr/local/bin/osqueryd",
        "pid": "97830"
      }
    ],
    "removed": [
      {
        "name": "osqueryd",
        "path": "/usr/local/bin/osqueryd",
        "pid": "97650"
      }
    ]
  },
  "name": "processes",
  "hostname": "hostname.local",
  "calendarTime": "Tue Sep 30 17:37:30 2014",
  "unixTime": "1412123850"
}
```

## Log transport

As previously mentioned, osqueryd's _status logs_ are written to the filesystem. If you want to aggregate and collect these logs from the filesystem, you can use an agent like fluentd, rsyslog, logstash, or Splunk universal forwarder.

For the _results logs_, if you prefer to directly transport your logs to something like Scribe or Flume, you can utilize the plugin architecture and [write a logger plugin](../development/logger-plugins).
(Or create a Github issue describing the log format/mechanism and we'll try our best to accommodate.)

## Unique host identification

If you need a way to uniquely identify hosts embedded into osqueryd's results log, then the `--host_identifier` flag is what you're looking for.
By default, `--host_identifier` is set to "hostname".
The host's hostname will be used as the host identifier in results logs.
If hostnames are not unique or consistent in your environment, you can launch osqueryd with `--host_identifier=uuid`.

On Linux, a new UUID will be generated and stored in RocksDB so that it persists across reboots. On OS X, this will attempt to use the hardware UUID and fail back to using a custom generated UUID if that fails.
